{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: packaging in f:\\work\\github\\social-network\\nlp-sentiment-analyzer\\nlp-2412122\\lib\\site-packages (from kagglehub) (24.2)\n",
      "Requirement already satisfied: requests in f:\\work\\github\\social-network\\nlp-sentiment-analyzer\\nlp-2412122\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Collecting tqdm (from kagglehub)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\work\\github\\social-network\\nlp-sentiment-analyzer\\nlp-2412122\\lib\\site-packages (from requests->kagglehub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\work\\github\\social-network\\nlp-sentiment-analyzer\\nlp-2412122\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\work\\github\\social-network\\nlp-sentiment-analyzer\\nlp-2412122\\lib\\site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\work\\github\\social-network\\nlp-sentiment-analyzer\\nlp-2412122\\lib\\site-packages (from requests->kagglehub) (2024.12.14)\n",
      "Requirement already satisfied: colorama in f:\\work\\github\\social-network\\nlp-sentiment-analyzer\\nlp-2412122\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Downloading kagglehub-0.3.6-py3-none-any.whl (51 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, kagglehub\n",
      "Successfully installed kagglehub-0.3.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\baong\\.cache\\kagglehub\\datasets\\jp797498e\\twitter-entity-sentiment-analysis\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os, sys, re\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"jp797498e/twitter-entity-sentiment-analysis\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_training.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = [f for f in os.listdir(Path(f'{path}').absolute()) if f.endswith('training.csv')]\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baong\\AppData\\Local\\Temp\\ipykernel_1656\\1227249578.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Sentiment'] = df['Sentiment'].replace('Positive'  ,  1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((74682, 4),\n",
       " Index(['Tweet_ID', 'Entity', 'Sentiment', 'Tweet_content'], dtype='object'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.setrecursionlimit(5000)\n",
    "script_location = Path('social-network/NLP-Sentiment-Analyzer').absolute()\n",
    "\n",
    "# Expanding the dispay of text sms column\n",
    "pd.set_option('display.max_colwidth', 1)\n",
    "\n",
    "csv_folder = [f for f in os.listdir(Path(f'{path}').absolute()) if f.endswith('training.csv')]\n",
    "data = []\n",
    "for f in csv_folder:\n",
    "    data.append(pd.read_csv(path + '/' + f, header=None))\n",
    "\n",
    "df = pd.concat(data, ignore_index=True)\n",
    "df.rename(columns={0:'Tweet_ID', 1: 'Entity', 2: 'Sentiment', 3: 'Tweet_content'}, inplace=True)\n",
    "df['Sentiment'] = df['Sentiment'].replace('Negative'  , -1)\n",
    "df['Sentiment'] = df['Sentiment'].replace('Irrelevant',  0)\n",
    "df['Sentiment'] = df['Sentiment'].replace('Neutral'   ,  0)\n",
    "df['Sentiment'] = df['Sentiment'].replace('Positive'  ,  1)\n",
    "df['Tweet_content'] = df['Tweet_content'].astype(str).fillna('')\n",
    "df.shape, df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74682 entries, 0 to 74681\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Tweet_ID       74682 non-null  int64 \n",
      " 1   Entity         74682 non-null  object\n",
      " 2   Sentiment      74682 non-null  int64 \n",
      " 3   Tweet_content  74682 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>im getting on borderlands and i will murder you all ,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>I am coming to the borders and I will kill you all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>im getting on borderlands and i will kill you all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>im coming on borderlands and i will murder you all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>im getting on borderlands 2 and i will murder you me all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>im getting into borderlands and i can murder you all,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2402</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>So I spent a few hours making something for fun. . . If you don't know I am a HUGE @Borderlands fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2402</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>So I spent a couple of hours doing something for fun... If you don't know that I'm a huge @ Borderlands fan and Maya is one of my favorite characters, I decided to make a wallpaper for my PC.. Here's the original picture compared to the creation I made:) Have fun! pic.twitter.com / mLsI5wf9Jg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2402</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>So I spent a few hours doing something for fun... If you don't know I'm a HUGE @ Borderlands fan and Maya is one of my favorite characters.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2402</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>So I spent a few hours making something for fun. . . If you don't know I am a HUGE RhandlerR fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet_ID       Entity  Sentiment  \\\n",
       "0  2401      Borderlands  1           \n",
       "1  2401      Borderlands  1           \n",
       "2  2401      Borderlands  1           \n",
       "3  2401      Borderlands  1           \n",
       "4  2401      Borderlands  1           \n",
       "5  2401      Borderlands  1           \n",
       "6  2402      Borderlands  1           \n",
       "7  2402      Borderlands  1           \n",
       "8  2402      Borderlands  1           \n",
       "9  2402      Borderlands  1           \n",
       "\n",
       "                                                                                                                                                                                                                                                                                           Tweet_content  \n",
       "0  im getting on borderlands and i will murder you all ,                                                                                                                                                                                                                                                  \n",
       "1  I am coming to the borders and I will kill you all,                                                                                                                                                                                                                                                    \n",
       "2  im getting on borderlands and i will kill you all,                                                                                                                                                                                                                                                     \n",
       "3  im coming on borderlands and i will murder you all,                                                                                                                                                                                                                                                    \n",
       "4  im getting on borderlands 2 and i will murder you me all,                                                                                                                                                                                                                                              \n",
       "5  im getting into borderlands and i can murder you all,                                                                                                                                                                                                                                                  \n",
       "6  So I spent a few hours making something for fun. . . If you don't know I am a HUGE @Borderlands fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg         \n",
       "7  So I spent a couple of hours doing something for fun... If you don't know that I'm a huge @ Borderlands fan and Maya is one of my favorite characters, I decided to make a wallpaper for my PC.. Here's the original picture compared to the creation I made:) Have fun! pic.twitter.com / mLsI5wf9Jg  \n",
       "8  So I spent a few hours doing something for fun... If you don't know I'm a HUGE @ Borderlands fan and Maya is one of my favorite characters.                                                                                                                                                            \n",
       "9  So I spent a few hours making something for fun. . . If you don't know I am a HUGE RhandlerR fan and Maya is one of my favorite characters. So I decided to make myself a wallpaper for my PC. . Here is the original image versus the creation I made :) Enjoy! pic.twitter.com/mLsI5wf9Jg            "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud as wordcloud\n",
    "from collections import Counter\n",
    "import streamlit as st\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.0-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\giab01\\fcv-dna\\shynh-group\\sg-20241215\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\giab01\\fcv-dna\\shynh-group\\sg-20241215\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\giab01\\fcv-dna\\shynh-group\\sg-20241215\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\giab01\\fcv-dna\\shynh-group\\sg-20241215\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Using cached scikit_learn-1.6.0-cp312-cp312-win_amd64.whl (11.1 MB)\n",
      "Installing collected packages: scikit-learn\n",
      "Successfully installed scikit-learn-1.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "        \n",
    "    \"\"\"   \n",
    "    # Remove HTML markup\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    # Replace n't at note\n",
    "    text = re.sub(r\"n't\", 'not', text)\n",
    "    # Remove emoticons\n",
    "    text = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', '', text)\n",
    "    # Remove any non-word character and digit\n",
    "    text = re.sub(r'[^A-Za-z ]+', '', text)\n",
    "    # Also Convert to lower case\n",
    "    text = (re.sub(r'[\\W]+', ' ', text.lower()))    \n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    \"\"\"Split a text into list of words and apply stemming technic\n",
    "    \n",
    "    \"\"\"\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "          \n",
    "def convert(sentiment):\n",
    "    \"\"\"Convert from 5 sentiment to 3 sentiment\n",
    "    \n",
    "    \"\"\"\n",
    "    if sentiment < 0:\n",
    "        sentiment = -1 # Negative\n",
    "    if sentiment == 0:\n",
    "        sentiment = 0 # Neutral\n",
    "    if sentiment > 0:\n",
    "        sentiment = 1 # Positive\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\baong\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "df['converted_sentiment'] = df['Sentiment'].apply(convert)\n",
    "df['preprocessed'] = df['Tweet_content'].apply(preprocessor)\n",
    "df['keyword_list'] = df['preprocessed'].apply(tokenizer_porter)\n",
    "pickle.dump(df, open(f'DataFrame.sav', 'wb')) ## Save Dataframe to .sav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_content</th>\n",
       "      <th>converted_sentiment</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>keyword_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>im getting on borderlands and i will murder you all ,</td>\n",
       "      <td>1</td>\n",
       "      <td>im getting on borderlands and i will murder you all</td>\n",
       "      <td>[im, get, on, borderland, and, i, will, murder, you, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>I am coming to the borders and I will kill you all,</td>\n",
       "      <td>1</td>\n",
       "      <td>i am coming to the borders and i will kill you all</td>\n",
       "      <td>[i, am, come, to, the, border, and, i, will, kill, you, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>im getting on borderlands and i will kill you all,</td>\n",
       "      <td>1</td>\n",
       "      <td>im getting on borderlands and i will kill you all</td>\n",
       "      <td>[im, get, on, borderland, and, i, will, kill, you, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>im coming on borderlands and i will murder you all,</td>\n",
       "      <td>1</td>\n",
       "      <td>im coming on borderlands and i will murder you all</td>\n",
       "      <td>[im, come, on, borderland, and, i, will, murder, you, all]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>im getting on borderlands 2 and i will murder you me all,</td>\n",
       "      <td>1</td>\n",
       "      <td>im getting on borderlands and i will murder you me all</td>\n",
       "      <td>[im, get, on, borderland, and, i, will, murder, you, me, all]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet_ID       Entity  Sentiment  \\\n",
       "0  2401      Borderlands  1           \n",
       "1  2401      Borderlands  1           \n",
       "2  2401      Borderlands  1           \n",
       "3  2401      Borderlands  1           \n",
       "4  2401      Borderlands  1           \n",
       "\n",
       "                                               Tweet_content  \\\n",
       "0  im getting on borderlands and i will murder you all ,       \n",
       "1  I am coming to the borders and I will kill you all,         \n",
       "2  im getting on borderlands and i will kill you all,          \n",
       "3  im coming on borderlands and i will murder you all,         \n",
       "4  im getting on borderlands 2 and i will murder you me all,   \n",
       "\n",
       "   converted_sentiment  \\\n",
       "0  1                     \n",
       "1  1                     \n",
       "2  1                     \n",
       "3  1                     \n",
       "4  1                     \n",
       "\n",
       "                                             preprocessed  \\\n",
       "0  im getting on borderlands and i will murder you all      \n",
       "1  i am coming to the borders and i will kill you all       \n",
       "2  im getting on borderlands and i will kill you all        \n",
       "3  im coming on borderlands and i will murder you all       \n",
       "4  im getting on borderlands and i will murder you me all   \n",
       "\n",
       "                                                    keyword_list  \n",
       "0  [im, get, on, borderland, and, i, will, murder, you, all]      \n",
       "1  [i, am, come, to, the, border, and, i, will, kill, you, all]   \n",
       "2  [im, get, on, borderland, and, i, will, kill, you, all]        \n",
       "3  [im, come, on, borderland, and, i, will, murder, you, all]     \n",
       "4  [im, get, on, borderland, and, i, will, murder, you, me, all]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Work\\GitHub\\social-network\\NLP-Sentiment-Analyzer\\NLP-2412122\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arenot', 'couldnot', 'didnot', 'doesnot', 'donot', 'hadnot', 'hasnot', 'havenot', 'isnot', 'mightnot', 'mustnot', 'neednot', 'shanot', 'shes', 'shouldnot', 'shouldve', 'thatll', 'wasnot', 'werenot', 'wonot', 'wouldnot', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9148423378188392\n",
      "confusion matrix:\n",
      " [[4005  336  128]\n",
      " [ 176 5887  189]\n",
      " [ 124  319 3773]]\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.90      0.91      4469\n",
      "           0       0.90      0.94      0.92      6252\n",
      "           1       0.92      0.89      0.91      4216\n",
      "\n",
      "    accuracy                           0.91     14937\n",
      "   macro avg       0.92      0.91      0.91     14937\n",
      "weighted avg       0.92      0.91      0.91     14937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model on non-converted sentiment\n",
    "stopword = stopwords.words('english')\n",
    "X = df['Tweet_content']\n",
    "y = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# tfid = TfidfVectorizer(stop_words=stop_words, tokenizer=tokenizer_porter, preprocessor=preprocessor)\n",
    "count = CountVectorizer(stop_words=stopword, preprocessor=preprocessor)\n",
    "\n",
    "# Construct pipeline\n",
    "clf = Pipeline([('vect', count), ('clf', LogisticRegression(random_state=0, max_iter=1000, n_jobs=6))])\n",
    "clf.fit(X, y)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "#Print Result\n",
    "print('accuracy:',accuracy_score(y_test, prediction))\n",
    "print('confusion matrix:\\n',confusion_matrix(y_test, prediction))\n",
    "print('classification report:\\n',classification_report(y_test, prediction))\n",
    "    \n",
    "# Save trained model to disk\n",
    "pickle.dump(clf, open('model1.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Work\\GitHub\\social-network\\NLP-Sentiment-Analyzer\\NLP-2412122\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arenot', 'couldnot', 'didnot', 'doesnot', 'donot', 'hadnot', 'hasnot', 'havenot', 'isnot', 'mightnot', 'mustnot', 'neednot', 'shanot', 'shes', 'shouldnot', 'shouldve', 'thatll', 'wasnot', 'werenot', 'wonot', 'wouldnot', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9148423378188392\n",
      "confusion matrix:\n",
      " [[4005  336  128]\n",
      " [ 176 5887  189]\n",
      " [ 124  319 3773]]\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.90      0.91      4469\n",
      "           0       0.90      0.94      0.92      6252\n",
      "           1       0.92      0.89      0.91      4216\n",
      "\n",
      "    accuracy                           0.91     14937\n",
      "   macro avg       0.92      0.91      0.91     14937\n",
      "weighted avg       0.92      0.91      0.91     14937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model on converted sentiment\n",
    "stopword = stopwords.words('english')\n",
    "X = df['Tweet_content']\n",
    "y2 = df['converted_sentiment']\n",
    "X_train, X_test, y_train_2, y_test_2 = train_test_split(X, y2, test_size=0.2, random_state=0)\n",
    "# tfid = TfidfVectorizer(stop_words=stop_words, tokenizer=tokenizer_porter, preprocessor=preprocessor)\n",
    "count = CountVectorizer(stop_words=stopword, preprocessor=preprocessor)\n",
    "\n",
    "# Construct pipeline\n",
    "clf = Pipeline([('vect', count), ('clf', LogisticRegression(random_state=0, max_iter=1000, n_jobs=6))])\n",
    "clf.fit(X, y2)\n",
    "prediction2 = clf.predict(X_test)\n",
    "\n",
    "#Print Result\n",
    "print('accuracy:',accuracy_score(y_test_2,prediction2))\n",
    "print('confusion matrix:\\n',confusion_matrix(y_test_2,prediction2))\n",
    "print('classification report:\\n',classification_report(y_test_2,prediction2))\n",
    "    \n",
    "# Save trained model to disk\n",
    "pickle.dump(clf, open('model2.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_validation.csv']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [f for f in os.listdir(Path(f'{path}').absolute()) if f.endswith('validation.csv')]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baong\\AppData\\Local\\Temp\\ipykernel_1656\\527537233.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df1['Sentiment'] = df1['Sentiment'].replace('Positive'  ,  1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1000, 4),\n",
       " Index(['Tweet_ID', 'Entity', 'Sentiment', 'Tweet_content'], dtype='object'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.setrecursionlimit(5000)\n",
    "script_location = Path('social-network/NLP-Sentiment-Analyzer').absolute()\n",
    "\n",
    "# Expanding the dispay of text sms column\n",
    "pd.set_option('display.max_colwidth', 1)\n",
    "\n",
    "csv_folder_test = [f for f in os.listdir(Path(f'{path}').absolute()) if f.endswith('validation.csv')]\n",
    "data_test = []\n",
    "for f in csv_folder_test:\n",
    "    data_test.append(pd.read_csv(path + '/' + f, header=None))\n",
    "\n",
    "df1 = pd.concat(data_test, ignore_index=True)\n",
    "df1.rename(columns={0:'Tweet_ID', 1: 'Entity', 2: 'Sentiment', 3: 'Tweet_content'}, inplace=True)\n",
    "df1['Sentiment'] = df1['Sentiment'].replace('Negative'  , -1)\n",
    "df1['Sentiment'] = df1['Sentiment'].replace('Irrelevant',  0)\n",
    "df1['Sentiment'] = df1['Sentiment'].replace('Neutral'   ,  0)\n",
    "df1['Sentiment'] = df1['Sentiment'].replace('Positive'  ,  1)\n",
    "df1['Tweet_content'] = df1['Tweet_content'].astype(str).fillna('')\n",
    "df1.shape, df1.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3364</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>0</td>\n",
       "      <td>I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom‚Äôs great auntie as ‚ÄòHayley can‚Äôt get out of bed‚Äô and told to his grandma, who now thinks I‚Äôm a lazy, terrible person ü§£</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>352</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>0</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects claims company acted like a 'drug dealer' bbc.co.uk/news/av/busine‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>-1</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it functions so poorly on my @SamsungUS Chromebook? üôÑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4371</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>-1</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking, it's a truly awful game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4433</td>\n",
       "      <td>Google</td>\n",
       "      <td>0</td>\n",
       "      <td>Now the President is slapping Americans in the face that he really did commit an unlawful act after his  acquittal! From Discover on Google vanityfair.com/news/2020/02/t‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>4891</td>\n",
       "      <td>GrandTheftAuto(GTA)</td>\n",
       "      <td>0</td>\n",
       "      <td>‚≠êÔ∏è Toronto is the arts and culture capital of Canada, it‚Äôs no wonder! If you want to start planning, be sure to check out our GTA Real Estate market report for Fall 2020, it has all the info you need to finally make a move! blog.remax.ca/toronto-housin‚Ä¶ twitter.com/kevinyoufool/s‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>4359</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>0</td>\n",
       "      <td>tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VIEWERS.\\n\\nI was one of those people who got hooked into csgo by watching tournaments first before playing the game. And seeing these players grew is like a netflix docu series for me. Can't wait for 2021.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2652</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>1</td>\n",
       "      <td>Today sucked so it‚Äôs time to drink wine n play borderlands until the sun comes up so I can hate myself all day tomorrow.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>8069</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>1</td>\n",
       "      <td>Bought a fraction of Microsoft today. Small wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>6960</td>\n",
       "      <td>johnson&amp;johnson</td>\n",
       "      <td>0</td>\n",
       "      <td>Johnson &amp; Johnson to stop selling talc baby powder in U.S. and Canada j.mp/3e1YtDV (Reuters) https://t.co/dsaUTgb5p9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tweet_ID               Entity  Sentiment  \\\n",
       "0    3364      Facebook             0           \n",
       "1    352       Amazon               0           \n",
       "2    8312      Microsoft           -1           \n",
       "3    4371      CS-GO               -1           \n",
       "4    4433      Google               0           \n",
       "..    ...         ...              ..           \n",
       "995  4891      GrandTheftAuto(GTA)  0           \n",
       "996  4359      CS-GO                0           \n",
       "997  2652      Borderlands          1           \n",
       "998  8069      Microsoft            1           \n",
       "999  6960      johnson&johnson      0           \n",
       "\n",
       "                                                                                                                                                                                                                                                                                 Tweet_content  \n",
       "0    I mentioned on Facebook that I was struggling for motivation to go for a run the other day, which has been translated by Tom‚Äôs great auntie as ‚ÄòHayley can‚Äôt get out of bed‚Äô and told to his grandma, who now thinks I‚Äôm a lazy, terrible person ü§£                                         \n",
       "1    BBC News - Amazon boss Jeff Bezos rejects claims company acted like a 'drug dealer' bbc.co.uk/news/av/busine‚Ä¶                                                                                                                                                                              \n",
       "2    @Microsoft Why do I pay for WORD when it functions so poorly on my @SamsungUS Chromebook? üôÑ                                                                                                                                                                                                \n",
       "3    CSGO matchmaking is so full of closet hacking, it's a truly awful game.                                                                                                                                                                                                                    \n",
       "4    Now the President is slapping Americans in the face that he really did commit an unlawful act after his  acquittal! From Discover on Google vanityfair.com/news/2020/02/t‚Ä¶                                                                                                                 \n",
       "..                                                                                                                                                                          ...                                                                                                                 \n",
       "995  ‚≠êÔ∏è Toronto is the arts and culture capital of Canada, it‚Äôs no wonder! If you want to start planning, be sure to check out our GTA Real Estate market report for Fall 2020, it has all the info you need to finally make a move! blog.remax.ca/toronto-housin‚Ä¶ twitter.com/kevinyoufool/s‚Ä¶  \n",
       "996  tHIS IS ACTUALLY A GOOD MOVE TOT BRING MORE VIEWERS.\\n\\nI was one of those people who got hooked into csgo by watching tournaments first before playing the game. And seeing these players grew is like a netflix docu series for me. Can't wait for 2021.                                 \n",
       "997  Today sucked so it‚Äôs time to drink wine n play borderlands until the sun comes up so I can hate myself all day tomorrow.                                                                                                                                                                   \n",
       "998  Bought a fraction of Microsoft today. Small wins.                                                                                                                                                                                                                                          \n",
       "999  Johnson & Johnson to stop selling talc baby powder in U.S. and Canada j.mp/3e1YtDV (Reuters) https://t.co/dsaUTgb5p9                                                                                                                                                                       \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(27.700000000000003)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Sentiment'][df1['Sentiment']==1].count() / df1.shape[0] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baong\\AppData\\Local\\Temp\\ipykernel_1656\\1065487642.py:38: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_test['Sentiment'] = df_test['Sentiment'].replace('Positive'  ,  1)\n",
      "f:\\Work\\GitHub\\social-network\\NLP-Sentiment-Analyzer\\NLP-2412122\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arenot', 'couldnot', 'didnot', 'doesnot', 'donot', 'hadnot', 'hasnot', 'havenot', 'isnot', 'mightnot', 'mustnot', 'neednot', 'shanot', 'shes', 'shouldnot', 'shouldve', 'thatll', 'wasnot', 'werenot', 'wonot', 'wouldnot', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\baong\\AppData\\Local\\Temp\\ipykernel_1656\\1065487642.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return int(model1.predict([phrase]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export Successfully!!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "script_location = Path('social-network/NLP-Sentiment-Analyzer').absolute()\n",
    "\n",
    "def load_model():\n",
    "    model1 = pickle.load(open('model1.sav', 'rb'))\n",
    "    return model1\n",
    "\n",
    "def predict(phrase):\n",
    "    return int(model1.predict([phrase]))\n",
    "\n",
    "def result(df_test):\n",
    "    result = []\n",
    "    for i in range(df_test.shape[0]):\n",
    "        if df_test['Sentiment'].iloc[i] == df_test['Sentiment_Test'].iloc[i]:\n",
    "            result.append('T')\n",
    "        else:\n",
    "            result.append('F')\n",
    "    df_test['Result'] = result\n",
    "    return df_test\n",
    "\n",
    "model1 = load_model()\n",
    "def main():  \n",
    "    csv_folder_test = [f for f in os.listdir(Path(f'{path}').absolute()) if f.endswith('validation.csv')]\n",
    "    data_test = []\n",
    "    for f in csv_folder_test:\n",
    "        data_test.append(pd.read_csv(path + '/' + f, header=None))\n",
    "\n",
    "    df_test = pd.concat(data_test, ignore_index=True)\n",
    "    df_test.rename(columns={0:'Tweet_ID', 1: 'Entity', 2: 'Sentiment', 3: 'Tweet_content'}, inplace=True)\n",
    "    df_test['Sentiment'] = df_test['Sentiment'].replace('Negative'  , -1)\n",
    "    df_test['Sentiment'] = df_test['Sentiment'].replace('Irrelevant',  0)\n",
    "    df_test['Sentiment'] = df_test['Sentiment'].replace('Neutral'   ,  0)\n",
    "    df_test['Sentiment'] = df_test['Sentiment'].replace('Positive'  ,  1)\n",
    "    df_test['Tweet_content'] = df_test['Tweet_content'].astype(str).fillna('')\n",
    "    df_test['Sentiment_Test'] = df_test['Tweet_content'].apply(predict)\n",
    "    try:\n",
    "        result(df_test)\n",
    "        df_test.to_csv('final_result.csv')\n",
    "        print('Export Successfully!!')\n",
    "    except:\n",
    "        print('Fail to export')\n",
    "        raise\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-2412122",
   "language": "python",
   "name": "nlp-2412122"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
